{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0sspW1ClMVG"
      },
      "outputs": [],
      "source": [
        "# Making some essential imports as Usual\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import re\n",
        "import cv2 as op\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from os import listdir\n",
        "import pandas as pd\n",
        "from keras.layers import Dense, Dropout, Input, add, Conv2D, BatchNormalization, MaxPooling2D, Conv2DTranspose,Activation, Concatenate\n",
        "from tensorflow import keras\n",
        "from time import time\n",
        "from tqdm import tqdm\n",
        "from keras import backend as K\n",
        "\n",
        "# plt.style.use('seaborn')\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install albumentations > /dev/null\n",
        "!pip install -U efficientnet==0.0.4\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "import keras\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-white')\n",
        "import seaborn as sns\n",
        "sns.set_style(\"white\")\n",
        "\n",
        "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
        "\n",
        "from skimage.transform import resize\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "from keras.losses import binary_crossentropy\n",
        "\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras import Model\n",
        "from keras.callbacks import  ModelCheckpoint\n",
        "from keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout,BatchNormalization\n",
        "from keras.layers import Conv2D, Concatenate, MaxPooling2D\n",
        "from keras.layers import UpSampling2D, Dropout, BatchNormalization\n",
        "from tqdm import tqdm_notebook\n",
        "from keras import initializers\n",
        "from keras import regularizers\n",
        "from keras import constraints\n",
        "from keras.utils import conv_utils\n",
        "from keras.utils.data_utils import get_file\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import ZeroPadding2D\n",
        "from keras.losses import binary_crossentropy\n",
        "import keras.callbacks as callbacks\n",
        "from keras.callbacks import Callback\n",
        "from keras.applications.xception import Xception\n",
        "from keras.layers import multiply\n"
      ],
      "metadata": {
        "id": "dvdFa6FtliUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils.data_utils import get_file"
      ],
      "metadata": {
        "id": "fpcHSTQ9ljtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%env SM_FRAMEWORK = tf.keras\n",
        "!pip install segmentation_models\n",
        "!pip install  albumentations"
      ],
      "metadata": {
        "id": "QdzLpzD2ln0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\n'.join(f'{m.__name__}=={m.__version__}' for m in globals().values() if getattr(m, '__version__', None)))"
      ],
      "metadata": {
        "id": "tPyd_ySQlqx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = plt.imread('../input/massachusetts-roads-dataset/tiff/train/10228675_15.tiff')\n",
        "print(img.shape)\n",
        "\n",
        "plt.imshow(img)"
      ],
      "metadata": {
        "id": "qPXZMBMnlvs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_data(PATH):\n",
        "    data = []\n",
        "    filenames = sorted(listdir(PATH))\n",
        "    for name in filenames:\n",
        "        data.append(PATH + '/' + name)\n",
        "    return data\n",
        "\n",
        "    \n",
        "TRAIN_PATH = '../input/massachusetts-roads-dataset/tiff/train'\n",
        "LABEL_PATH = '../input/massachusetts-roads-dataset/tiff/train_labels'\n",
        "\n",
        "sat_image = build_data(TRAIN_PATH)\n",
        "mask_image = build_data(LABEL_PATH)\n",
        "\n",
        "VAL_TRAIN_PATH = '../input/massachusetts-roads-dataset/tiff/val'\n",
        "VAL_LABEL_PATH = '../input/massachusetts-roads-dataset/tiff/val_labels'\n",
        "\n",
        "VAL_sat_image = build_data(VAL_TRAIN_PATH)\n",
        "VAL_mask_image = build_data(VAL_LABEL_PATH)"
      ],
      "metadata": {
        "id": "AkJ8rrKTlx9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_PATH = '../input/massachusetts-roads-dataset/tiff/test'\n",
        "TEST_LABEL_PATH = '../input/massachusetts-roads-dataset/tiff/test_labels'\n",
        "\n",
        "TEST_sat_image = build_data(TEST_PATH)\n",
        "TEST_mask_image = build_data(TEST_LABEL_PATH)"
      ],
      "metadata": {
        "id": "Rmy7SloYlz82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(sat_image), len(mask_image), len(VAL_sat_image), len(VAL_mask_image ),len(TEST_sat_image), len(TEST_mask_image )"
      ],
      "metadata": {
        "id": "oxWuamYbl2tW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({\n",
        "    'Image' : sat_image,\n",
        "    'Mask' : mask_image\n",
        "})"
      ],
      "metadata": {
        "id": "MsDzL-gXl40V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_df = pd.DataFrame({\n",
        "    'Val_Image' : VAL_sat_image,\n",
        "    'Val_Mask' : VAL_mask_image\n",
        "})"
      ],
      "metadata": {
        "id": "jMQE_vXVl7ON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.DataFrame({\n",
        "    'Test_Image' : TEST_sat_image,\n",
        "    'Test_Mask' : TEST_mask_image\n",
        "})"
      ],
      "metadata": {
        "id": "vM_HBYljl89t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import albumentations as A\n",
        "transform = A.Compose([\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.Blur(blur_limit = 5, p = 0.85), \n",
        "#     A.RandomCrop(height = 512, width = 512, p = 1)\n",
        "])"
      ],
      "metadata": {
        "id": "0y59uHZ1mDA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = (256, 256)\n",
        "\n",
        "def modify_mask(mask):\n",
        "    mask = np.expand_dims(mask, axis = 2)\n",
        "    t_mask = np.zeros(mask.shape)\n",
        "    np.place(t_mask[:, :, 0], mask[:, :, 0] >=100, 1)\n",
        "    return t_mask\n",
        "\n",
        "def map_function(img, mask):\n",
        "    img, mask = plt.imread(img.decode()), plt.imread(mask.decode())\n",
        "    img = op.resize(img, IMG_SIZE)\n",
        "    mask = modify_mask(op.resize(mask, IMG_SIZE))\n",
        "    \n",
        "    img = img/255.0\n",
        "    transformed = transform(image=img, mask=mask)\n",
        "    img = transformed['image']\n",
        "    mask = transformed['mask']\n",
        "\n",
        "    #mask = modify_mask(mask)\n",
        "    \n",
        "    return img.astype(np.float64), mask.astype(np.float64)"
      ],
      "metadata": {
        "id": "cxjlFnlamGdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 16\n",
        "\n",
        "def create_dataset(data):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((data['Image'], data['Mask']))\n",
        "    dataset = dataset.shuffle(1000)\n",
        "    dataset = dataset.map(lambda img, mask : tf.numpy_function(\n",
        "                    map_function, [img, mask], [tf.float64, tf.float64]),\n",
        "                    num_parallel_calls = tf.data.experimental.AUTOTUNE).batch(BATCH_SIZE)\n",
        "\n",
        "    dataset = dataset.prefetch(buffer_size = tf.data.experimental.AUTOTUNE)\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "xeKA6MfXmL6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 16\n",
        "\n",
        "def create_dataset1(data):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((data['Val_Image'], data['Val_Mask']))\n",
        "    dataset = dataset.shuffle(1000)\n",
        "    dataset = dataset.map(lambda img, mask : tf.numpy_function(\n",
        "                    map_function, [img, mask], [tf.float64, tf.float64]),\n",
        "                    num_parallel_calls = tf.data.experimental.AUTOTUNE).batch(BATCH_SIZE)\n",
        "\n",
        "    dataset = dataset.prefetch(buffer_size = tf.data.experimental.AUTOTUNE)\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "SxyQSD8LmW5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 16\n",
        "\n",
        "def create_dataset2(data):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((data['Test_Image'], data['Test_Mask']))\n",
        "    dataset = dataset.shuffle(1000)\n",
        "    dataset = dataset.map(lambda img, mask : tf.numpy_function(\n",
        "                    map_function, [img, mask], [tf.float64, tf.float64]),\n",
        "                    num_parallel_calls = tf.data.experimental.AUTOTUNE).batch(BATCH_SIZE)\n",
        "\n",
        "    dataset = dataset.prefetch(buffer_size = tf.data.experimental.AUTOTUNE)\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "SNJwfyTQmYw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = create_dataset(df)\n",
        "img, mask = next(iter(train_dataset))\n",
        "img.shape, mask.shape"
      ],
      "metadata": {
        "id": "C6TlC0ArmayN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = create_dataset1(val_df)\n",
        "val_img, val_mask = next(iter(val_dataset))\n",
        "val_img.shape, val_mask.shape"
      ],
      "metadata": {
        "id": "8QU0oTQ6mc2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = create_dataset2(test_df)\n",
        "test_img, test_mask = next(iter(test_dataset))\n",
        "test_img.shape, test_mask.shape"
      ],
      "metadata": {
        "id": "74gWEcYHmex0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}