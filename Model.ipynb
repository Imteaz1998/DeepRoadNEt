{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xe-4l4KlnY87"
      },
      "outputs": [],
      "source": [
        "# Making some essential imports as Usual\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import re\n",
        "import cv2 as op\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from os import listdir\n",
        "import pandas as pd\n",
        "from keras.layers import Dense, Dropout, Input, add, Conv2D, BatchNormalization, MaxPooling2D, Conv2DTranspose,Activation, Concatenate\n",
        "from tensorflow import keras\n",
        "from time import time\n",
        "from tqdm import tqdm\n",
        "from keras import backend as K\n",
        "\n",
        "# plt.style.use('seaborn')\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install albumentations > /dev/null\n",
        "!pip install -U efficientnet==0.0.4\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "import keras\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-white')\n",
        "import seaborn as sns\n",
        "sns.set_style(\"white\")\n",
        "\n",
        "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
        "\n",
        "from skimage.transform import resize\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "from keras.losses import binary_crossentropy\n",
        "\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras import Model\n",
        "from keras.callbacks import  ModelCheckpoint\n",
        "from keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout,BatchNormalization\n",
        "from keras.layers import Conv2D, Concatenate, MaxPooling2D\n",
        "from keras.layers import UpSampling2D, Dropout, BatchNormalization\n",
        "from tqdm import tqdm_notebook\n",
        "from keras import initializers\n",
        "from keras import regularizers\n",
        "from keras import constraints\n",
        "from keras.utils import conv_utils\n",
        "from keras.utils.data_utils import get_file\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import ZeroPadding2D\n",
        "from keras.losses import binary_crossentropy\n",
        "import keras.callbacks as callbacks\n",
        "from keras.callbacks import Callback\n",
        "from keras.applications.xception import Xception\n",
        "from keras.layers import multiply"
      ],
      "metadata": {
        "id": "CFrLdVlJneSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "import keras\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-white')\n",
        "import seaborn as sns\n",
        "sns.set_style(\"white\")\n",
        "\n",
        "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
        "\n",
        "from skimage.transform import resize\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "from keras.losses import binary_crossentropy\n",
        "\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras import Model\n",
        "from keras.callbacks import  ModelCheckpoint\n",
        "from keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout,BatchNormalization\n",
        "from keras.layers import Conv2D, Concatenate, MaxPooling2D\n",
        "from keras.layers import UpSampling2D, Dropout, BatchNormalization\n",
        "from tqdm import tqdm_notebook\n",
        "from keras import initializers\n",
        "from keras import regularizers\n",
        "from keras import constraints\n",
        "from keras.utils import conv_utils\n",
        "from keras.utils.data_utils import get_file\n",
        "\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import ZeroPadding2D\n",
        "from keras.losses import binary_crossentropy\n",
        "import keras.callbacks as callbacks\n",
        "from keras.callbacks import Callback\n",
        "from keras.applications.efficientnet import EfficientNetB7\n",
        "from keras.layers import multiply\n",
        "\n",
        "\n",
        "from keras import optimizers\n",
        "\n",
        "from keras.utils.generic_utils import get_custom_objects\n",
        "\n",
        "\n",
        "from keras.engine.training import Model\n",
        "from keras.layers.convolutional import Conv2D, UpSampling2D, Conv2DTranspose\n",
        "from keras.layers.core import Activation, SpatialDropout2D\n",
        "from keras.layers.merge import concatenate\n",
        "\n",
        "from keras.layers.pooling import MaxPooling2D\n",
        "from keras.layers import Input,Dropout,BatchNormalization,Activation,Add\n",
        "from keras.regularizers import l2\n",
        "from keras.layers.core import Dense, Lambda\n",
        "from keras.layers.merge import concatenate, add\n",
        "from keras.layers import GlobalAveragePooling2D, Reshape, Dense, multiply, Permute\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "import glob\n",
        "import shutil\n",
        "import os\n",
        "import random\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "I_INm9HknjAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convolution_block(x, filters, size, strides=(1,1), padding='same', activation=True):\n",
        "    x = Conv2D(filters, size, strides=strides, padding=padding)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    if activation == True:\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "    return x\n",
        "\n",
        "def residual_block(blockInput, num_filters=16):\n",
        "    x = LeakyReLU(alpha=0.1)(blockInput)\n",
        "    x = BatchNormalization()(x)\n",
        "    blockInput = BatchNormalization()(blockInput)\n",
        "    x = convolution_block(x, num_filters, (3,3) )\n",
        "    x = convolution_block(x, num_filters, (3,3), activation=False)\n",
        "    x = Add()([x, blockInput])\n",
        "    return x"
      ],
      "metadata": {
        "id": "SGz6xNr-nl41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def UEfficientNet(input_shape=(None, None, 3),dropout_rate=0.1):\n",
        "\n",
        "    backbone = tf.keras.applications.EfficientNetB7(weights='imagenet',\n",
        "                            include_top=False,\n",
        "                            input_shape=input_shape)\n",
        "    input = backbone.input\n",
        "    start_neurons = 8\n",
        "\n",
        "    conv4 = backbone.layers[342].output\n",
        "    conv4 = LeakyReLU(alpha=0.1)(conv4)\n",
        "    pool4 = MaxPooling2D((2, 2))(conv4)\n",
        "    pool4 = Dropout(dropout_rate)(pool4)\n",
        "    \n",
        "     # Middle\n",
        "    convm = Conv2D(start_neurons * 32, (3, 3), activation=None, padding=\"same\")(pool4)\n",
        "    convm = residual_block(convm,start_neurons * 32)\n",
        "    convm = residual_block(convm,start_neurons * 32)\n",
        "    convm = LeakyReLU(alpha=0.1)(convm)\n",
        "    \n",
        "    deconv4 = Conv2DTranspose(start_neurons * 16, (3, 3), strides=(2, 2), padding=\"same\")(convm)\n",
        "    deconv4_up1 = Conv2DTranspose(start_neurons * 16, (3, 3), strides=(2, 2), padding=\"same\")(deconv4)\n",
        "    deconv4_up2 = Conv2DTranspose(start_neurons * 16, (3, 3), strides=(2, 2), padding=\"same\")(deconv4_up1)\n",
        "    deconv4_up3 = Conv2DTranspose(start_neurons * 16, (3, 3), strides=(2, 2), padding=\"same\")(deconv4_up2)\n",
        "    uconv4 = concatenate([deconv4, conv4])\n",
        "    uconv4 = Dropout(dropout_rate)(uconv4) \n",
        "    \n",
        "    uconv4 = Conv2D(start_neurons * 16, (3, 3), activation=None, padding=\"same\")(uconv4)\n",
        "    uconv4 = residual_block(uconv4,start_neurons * 16)\n",
        "#     uconv4 = residual_block(uconv4,start_neurons * 16)\n",
        "    uconv4 = LeakyReLU(alpha=0.1)(uconv4)  #conv1_2\n",
        "    \n",
        "    deconv3 = Conv2DTranspose(start_neurons * 8, (3, 3), strides=(2, 2), padding=\"same\")(uconv4)\n",
        "    deconv3_up1 = Conv2DTranspose(start_neurons * 8, (3, 3), strides=(2, 2), padding=\"same\")(deconv3)\n",
        "    deconv3_up2 = Conv2DTranspose(start_neurons * 8, (3, 3), strides=(2, 2), padding=\"same\")(deconv3_up1)\n",
        "    conv3 = backbone.layers[154].output\n",
        "    uconv3 = concatenate([deconv3,deconv4_up1, conv3])    \n",
        "    uconv3 = Dropout(dropout_rate)(uconv3)\n",
        "    \n",
        "    uconv3 = Conv2D(start_neurons * 8, (3, 3), activation=None, padding=\"same\")(uconv3)\n",
        "    uconv3 = residual_block(uconv3,start_neurons * 8)\n",
        "#     uconv3 = residual_block(uconv3,start_neurons * 8)\n",
        "    uconv3 = LeakyReLU(alpha=0.1)(uconv3)\n",
        "\n",
        "    deconv2 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"same\")(uconv3)\n",
        "    deconv2_up1 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"same\")(deconv2)\n",
        "    conv2 = backbone.layers[92].output\n",
        "    uconv2 = concatenate([deconv2,deconv3_up1,deconv4_up2, conv2])\n",
        "        \n",
        "    uconv2 = Dropout(0.1)(uconv2)\n",
        "    uconv2 = Conv2D(start_neurons * 4, (3, 3), activation=None, padding=\"same\")(uconv2)\n",
        "    uconv2 = residual_block(uconv2,start_neurons * 4)\n",
        "#     uconv2 = residual_block(uconv2,start_neurons * 4)\n",
        "    uconv2 = LeakyReLU(alpha=0.1)(uconv2)\n",
        "    \n",
        "    deconv1 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding=\"same\")(uconv2)\n",
        "    conv1 = backbone.layers[30].output\n",
        "    uconv1 = concatenate([deconv1,deconv2_up1,deconv3_up2,deconv4_up3, conv1])\n",
        "    \n",
        "    uconv1 = Dropout(0.1)(uconv1)\n",
        "    uconv1 = Conv2D(start_neurons * 2, (3, 3), activation=None, padding=\"same\")(uconv1)\n",
        "    uconv1 = residual_block(uconv1,start_neurons * 2)\n",
        "#     uconv1 = residual_block(uconv1,start_neurons * 2)\n",
        "    uconv1 = LeakyReLU(alpha=0.1)(uconv1)\n",
        "    \n",
        "    uconv0 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"same\")(uconv1)   \n",
        "    uconv0 = Dropout(0.1)(uconv0)\n",
        "    uconv0 = Conv2D(start_neurons * 1, (3, 3), activation=None, padding=\"same\")(uconv0)\n",
        "    uconv0 = residual_block(uconv0,start_neurons * 1)\n",
        "#     uconv0 = residual_block(uconv0,start_neurons * 1)\n",
        "    uconv0 = LeakyReLU(alpha=0.1)(uconv0)\n",
        "    \n",
        "    uconv0 = Dropout(dropout_rate/2)(uconv0)\n",
        "    output_layer = Conv2D(1, (1,1), padding=\"same\", activation=\"sigmoid\")(uconv0)    \n",
        "    \n",
        "    model = Model(input, output_layer)\n",
        "    model.name = 'u-xception'\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "LUbLxMbPno_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "K.clear_session()\n",
        "img_size = 256\n",
        "model = EfficientNetB7(input_shape=(img_size,img_size,3))"
      ],
      "metadata": {
        "id": "oe1GngBZnq7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "wB6kshVonsUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adam = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(loss=weighted_BCE_loss, optimizer=adam, metrics=[accuracy,dice_coe, numpy_mean_iou])"
      ],
      "metadata": {
        "id": "lEnt1gKSn19n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filepath=\"eff7_resdual.h5\"\n",
        "checkpoint = callbacks.ModelCheckpoint(filepath, monitor='val_accuracy',save_best_only=True, mode='max',verbose=1)\n",
        "callbacks_list = [checkpoint]"
      ],
      "metadata": {
        "id": "Q4iscoYCn4T2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hist=model.fit(\n",
        "    train_dataset,\n",
        "    \n",
        "    validation_data=val_dataset,\n",
        "    callbacks = callbacks_list,\n",
        "    \n",
        "    epochs = 200\n",
        ")"
      ],
      "metadata": {
        "id": "GB6HuXcQn4MN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
